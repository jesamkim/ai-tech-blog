# 2026-02-09

## 🎉 GCP Interview Invitation

**Status:** 첫 인터뷰 초대 받음

**From:** Daeun (Dana) Jeong <danajeong@google.com>  
**Position:** Customer Engineer, Cloud AI Platform  
**Date Received:** 2026-02-09 (16:48 KST)

**Interview Process:**
1. Round 1: RRK (Role-Related Knowledge) - 45분, Video Call
2. Round 2: GCA (General Cognitive Ability) - 45분, Video Call
3. Round 3: G&L (Googleyness & Leadership) - 45분, Video Call
4. Round 4: Presentation - 45분, Video Call

**⚠️ 각 라운드는 elimination round** (한 번 떨어지면 끝)

**준비 자료:**
- Interview Prep Guide (AI/ML 파트)
- YouTube 영상:
  - GCA: https://www.youtube.com/watch?v=eIMR82oO2Dc
  - Googleyness: https://www.youtube.com/watch?v=uPjeKSDuoGw&t=66s
  - Leadership: https://www.youtube.com/watch?v=2Cr3-et4xkI&t=158s
- Further guide: https://www.google.com/about/careers/applications/candidate-prep/ce

**⚠️ 주의사항:**
- 인터뷰 중 AI 사용 금지 (적발 시 탈락)

---

## 📅 Jay's Availability Response

**Sent:** 2026-02-09

Jay가 제안한 일정 (KST):
- **Feb 11 (Wed):** 13:30–17:00
- **Feb 12 (Thu):** 09:00–11:00
- **Feb 13 (Fri):** 09:00–17:00

**Feb 19-20:** 휴가 예정
- 이번 주 내로 진행 선호
- 필요 시 Feb 19 오후 가능

---

## 📝 RRK 인터뷰 예상 질문 (웹 조사 결과)

### 1. GenAI & LLM (출제 확률 99%)
- **RAG vs Fine-tuning**: 특정 도메인 챗봇 구축 시 어떤 방식? 비용/구현/보안 비교
- **Hallucination 제어**: 금융권처럼 정확도 중요한 경우 LLM 환각 줄이는 아키텍처
- **Context Window**: Gemini 1.5 Pro의 긴 컨텍스트가 있는데 왜 RAG 필요한가?
- **Vector Search**: 수십억 벡터 데이터에서 Latency 줄이면서 정확도 유지하는 인덱싱 기법 (Vertex AI Vector Search)

### 2. ML System Design (화이트보딩)
- **추천 시스템**: 이커머스 실시간 개인화 추천 시스템 (BigQuery ML → Vertex AI Training → Matching Engine)
- **MLOps 파이프라인**: Model Drift 감지 + 재학습 자동화 (CI/CD/CT)
- **검색 시스템**: 사내 문서 검색 (OCR → 검색 결과)

### 3. GCP vs AWS (Competitive)
- **SageMaker vs Vertex AI**: Vertex AI의 강점은?
- **Migration**: AWS LLM 워크로드를 GCP로 이관 시 걸림돌 + 해결책

### 4. Cloud Fundamentals
- **VPC & Networking**: VPC Peering vs Shared VPC, 멀티 프로젝트에서 AI 학습 데이터 공유
- **IAM & Security**: 데이터 과학자에게 Vertex AI 접근 권한 (Least Privilege)

### 💡 Jay의 경험 매핑 전략
- **RAG vs Fine-tuning** → 삼성물산 1.9PB 건설 데이터 프로젝트 (매일 업데이트 = RAG 선택)
- **MLOps/Drift** → 롯데ON Dynamic A/B Testing + Multi-Armed Bandit (자동 트래픽 전환)
- **AWS 비교** → AWS는 파편화(Bedrock, SageMaker, Q), GCP는 Vertex AI 통합 (Agent Builder + Search 연결 용이)

---

## 📚 답변 가이드 (AWS → GCP 브릿지 전략)

### Q1. RAG vs Fine-tuning
**논리**: 데이터 실시간성 + 환각 제어 → RAG / 도메인 특화 어조 + 복잡한 지시 → Fine-tuning

**AWS 경험**:
- 삼성물산: Amazon Bedrock + Knowledge Base (건설 규정 매일 변경)
- PEFT(LoRA): SageMaker JumpStart로 Llama 모델 튜닝 고려

**GCP Bridge**: Vertex AI Search (RAG) + Vertex AI Pipelines (튜닝 자동화)

---

### Q2. Hallucination 줄이기
**논리**: 프롬프트 엔지니어링 + 데이터 그라운딩 + 후처리(Guardrails)

**AWS 경험**:
- CoT(Chain of Thought) 프롬프팅
- Amazon Bedrock Guardrails (PII/유해 콘텐츠 차단)
- Citation 표기 시스템

**GCP Bridge**: Vertex AI Grounding service (검색 기반 신뢰성 검증)

---

### Q3. 실시간 추천 시스템 설계
**AWS 아키텍처**:
- 수집: Kinesis Data Streams (Clickstream)
- 학습: SageMaker (Object2Vec/DeepFM) + Feature Store
- 서빙: SageMaker Endpoints (Auto-scaling) + API Gateway + Lambda

**GCP Bridge**: Pub/Sub → Dataflow → Vertex AI Matching Engine (Vector Search)

---

### Q4. Data Drift 감지/해결
**AWS 경험**:
- SageMaker Model Monitor (Baseline vs 실시간 분포 차이)
- EventBridge 트리거 → SageMaker Pipelines 자동 재학습

**GCP Bridge**: Vertex AI Model Monitoring

---

### Q5. 온프레미스 → 클라우드 마이그레이션 (보안/속도)
**AWS 아키텍처**:
- Direct Connect (전용선)
- S3 + VPC Endpoint (퍼블릭 인터넷 회피)
- KMS 암호화

**GCP Bridge**: Cloud Interconnect + VPC Service Controls

---

### ⚡ 인터뷰 전략
1. **AWS 경험을 당당하게** - "AWS에서는 이렇게 했습니다" (억지로 GCP 용어 쓰지 말 것)
2. **GCP 브릿지 한 문장** - "GCP에서는 [서비스명]이 대응되는 것으로 알고 있습니다"
3. **Why에 집중** - "운영 오버헤드를 줄이기 위해 Managed Service 선택" (의사결정 근거)

---

### Q6. Feature Store란 무엇이며 왜 사용하나?
**정의**: ML 모델 학습/추론에 사용되는 Feature를 중앙에서 관리, 저장, 서빙하는 저장소
- **Offline Store**: 대용량/배치 학습용
- **Online Store**: 저지연/실시간 추론용

**핵심 이점**:
1. **Training-Serving Skew 해결** - 학습과 서빙 로직 일원화
2. **재사용성** - 다른 팀이 피처 검색/사용 가능 (중복 개발 방지)
3. **Point-in-Time Correctness** - 특정 시점 스냅샷 조회 (Data Leakage 방지)

**Cloud Mapping**:
- AWS: SageMaker Feature Store (S3 + DynamoDB)
- GCP: Vertex AI Feature Store (BigQuery + Bigtable)

---

### Q7. B2C 챗봇 트래픽 경합 해결 방법
**핵심 전략**: Queue-based Load Leveling + Throttling

**아키텍처 단계**:
1. **Throttling** (API Gateway) - 사용자별/IP별 RPS 제한
2. **비동기 큐** (SQS/Pub/Sub) - 요청을 큐에 넣고 202 Accepted 응답
3. **Worker Pool** - Auto-scaling으로 워커 증설 (KEDA 기반 K8s)
4. **응답 전달** - Redis 저장 + WebSocket/SSE로 실시간 알림

**추가 최적화**:
- **Semantic Caching**: 유사 질문 임베딩으로 캐시 답변 반환
- **Degradation**: 대기 길어지면 가벼운 모델(Gemma/Llama 8B)로 라우팅

**요약 답변**: "API Gateway에서 Throttling으로 입구 방어, 내부는 메시지 큐와 Worker 패턴으로 부하 평준화, Semantic Cache로 LLM 부하 최소화"

---

## 🔍 추가 리서치 결과 (웹 검색)

### 📌 실제 인터뷰 경험담 (dev.to)
- RRK 라운드에서 **상황 질문(Situational)** 빈도 높음
- "stakeholder 우선순위를 어떻게 정하고 defend할 것인가" 단골 질문
- **Hypervisor, VMware, Containerization** 같은 인프라 기본기도 중요
- 면접관이 "tech journey가 이제 시작 단계"라고 평가한 경우 탈락

### 📌 RRK 인터뷰 구조 (practiceinterviews.com)
**3가지 유형의 질문:**
1. **Behavioral**: "고객에게 모더나이제이션 기회를 찾아준 경험"
2. **Hypothetical**: "고객 기대치가 GCP로 달성 불가능할 때 어떻게 대응?"
3. **Technical**: "Netflix는 클라우드에서 어떻게 작동하나?" (시스템 설계)

### 📌 Hypothetical 질문 접근법
- **Clarifying Questions** (명확화 질문) 필수
- **Frameworks** 사용 (Goals, Data, Scope, Requirements, Training, Partnerships)
- **Assumptions** 미리 설정
- **GCP 제품 언급** (Vertex AI, BigQuery, Dataflow 등) 가산점

---

## 🆕 추가 예상 질문 (Q8~Q14)

### Q8. 고객 기대치 vs GCP 한계
**질문**: "고객의 요구사항이 GCP로 구현 불가능할 때 어떻게 대응하겠는가?"

**답변 전략**:
- ✅ **솔직함 우선** - "현재 GCP로는 X 기능이 어렵습니다"
- ✅ **대안 제시** - "하지만 Y 방식으로 80% 목표 달성 가능"
- ✅ **로드맵 공유** - "향후 Z 기능이 출시 예정" (내부 roadmap 있으면)
- ✅ **Partnership** - "필요하면 3rd-party 솔루션과 통합"

**Jay의 경험 활용**: AWS SA로서 "Bedrock이 지원 안 하는 모델"을 SageMaker JumpStart로 보완한 경험

---

### Q9. Netflix 같은 서비스를 클라우드에 설계
**질문**: "Netflix는 클라우드에서 어떻게 작동하는가?"

**답변 구조**:
1. **전역 콘텐츠 배포** (CDN)
   - GCP: Cloud CDN + Media CDN
   - AWS: CloudFront
2. **마이크로서비스 아키텍처** (컨테이너)
   - GCP: GKE (Google Kubernetes Engine)
   - AWS: EKS
3. **추천 시스템** (ML)
   - GCP: Vertex AI + BigQuery ML
   - AWS: SageMaker + Personalize
4. **실시간 스트리밍**
   - GCP: Pub/Sub + Dataflow
   - AWS: Kinesis + Lambda

**Jay의 강점**: 롯데ON 추천 시스템 경험으로 #3 섹션 깊이 있게 설명 가능

---

### Q10. 고객이 클라우드 네이티브 개발 방법론 거부 시
**질문**: "고객이 클라우드 네이티브를 도입하길 주저할 때 어떻게 설득하겠는가?"

**답변 전략**:
- ✅ **ROI 수치화** - "배포 시간 X시간 → Y분 단축" (구체적 사례)
- ✅ **점진적 전환** - "모놀리스 전체가 아닌 일부 API부터 컨테이너화"
- ✅ **리스크 완화** - "Pilot 프로젝트로 먼저 검증"
- ✅ **동종 업계 사례** - "같은 업계 X사가 Y개월 만에 성공"

**Jay의 경험**: 삼성물산 레거시 시스템을 RAG로 현대화한 경험

---

### Q11. Vertex AI 하이퍼파라미터 튜닝
**질문**: "Vertex AI의 하이퍼파라미터 튜닝 기능을 설명하고, 언제 사용하나?"

**답변**:
- **기능**: Vertex AI Training + Vizier (Bayesian Optimization)
- **언제 사용**: 모델 정확도가 충분히 나오지 않을 때, 하지만 시간/비용 제약 있을 때
- **대안**: Optuna, Hyperopt 같은 오픈소스 (더 커스터마이징 가능)

**AWS Mapping**: SageMaker Automatic Model Tuning

---

### Q12. Kubeflow vs Vertex AI Pipelines
**질문**: "MLOps 파이프라인 구축 시 Kubeflow와 Vertex AI Pipelines 중 뭘 선택하나?"

**답변**:
- **Vertex AI Pipelines** (Managed):
  - ✅ GCP 네이티브 서비스와 통합 용이 (BigQuery, Cloud Storage)
  - ✅ 운영 오버헤드 낮음
  - ❌ 커스터마이징 제한적
- **Kubeflow** (Self-managed):
  - ✅ 멀티 클라우드/온프레미스 이식 가능
  - ✅ Katib (하이퍼파라미터 튜닝) 같은 고급 기능
  - ❌ 운영 부담 큼 (K8s 관리 필요)

**추천**: "GCP 올인이고 빠른 출시가 중요하면 Vertex AI Pipelines, 하이브리드 환경이면 Kubeflow"

**AWS 경험**: "SageMaker Pipelines vs Airflow 선택과 유사"

---

### Q13. 멀티 프로젝트 환경에서 ML 데이터 공유
**질문**: "3개 팀이 각각 GCP 프로젝트를 쓰는데, 학습 데이터를 안전하게 공유하려면?"

**답변**:
1. **Shared VPC** (권장)
   - 중앙 호스트 프로젝트에 VPC 생성
   - 서비스 프로젝트들이 subnet 공유
   - IAM으로 세밀한 권한 제어
2. **VPC Peering** (대안)
   - 프로젝트 간 1:1 피어링
   - 관리 복잡도 높음 (N개 프로젝트면 N*(N-1)/2 피어링)
3. **BigQuery Datasets 공유**
   - Cross-project authorized views
   - Column-level security

**AWS 경험**: "AWS Organizations + Resource Access Manager와 유사"

---

### Q14. 스테이크홀더 우선순위 충돌
**질문**: "여러 고객이 동시에 긴급 지원을 요청했다. 어떻게 우선순위를 정하나?"

**답변 프레임워크**:
1. **Impact** (비즈니스 영향도)
   - Revenue at risk
   - 사용자 수
   - 브랜드 이미지
2. **Urgency** (긴급도)
   - SLA 위반 여부
   - 데드라인 (계약, 규제)
3. **Complexity** (해결 난이도)
   - Quick win 먼저 처리 가능한가
4. **Communication**
   - 대기 중인 고객에게 명확한 타임라인 제공
   - "2시간 후 업데이트 드리겠습니다" (기대치 관리)

**예시 답변**: "제조업 고객의 생산 라인 다운(P1)을 먼저 처리하고, 리테일 고객의 AI 모델 튜닝(P2)은 다음 날 오전으로 조정하되, 임시 workaround를 제공"

---

---

## 📝 인터뷰 준비 작업 (2026-02-09 오후)

### 작업 내용
1. **RRK 예상 질문 리서치**
   - 웹 검색 (Reddit, Glassdoor, dev.to, practiceinterviews.com)
   - GenAI & LLM, ML System Design, Hypothetical, GCP 네이티브 질문 수집
   - 총 15개 기술 질문 (Q1-Q15) 정리

2. **Notion 인터뷰 준비 페이지 생성**
   - 페이지: https://www.notion.so/GCP-CE-AI-ML-RRK-AWS-3029b4bfd0d281bdb66de19ce0452461
   - 모든 답변 AWS 경험 기반 + GCP 매핑 서비스
   - Jay 요청으로 PEFT (Q15) 추가
   - Jay 요청으로 Googleyness & Leadership (Q16-Q20) 추가

3. **최종 20개 질문 완성**
   - Q1-Q4: GenAI & LLM (RAG, Hallucination, 추천, Drift)
   - Q5-Q7: 인프라 (마이그레이션, Feature Store, 트래픽)
   - Q8-Q10: Hypothetical (고객 기대치, Netflix, 설득)
   - Q11-Q13: GCP 네이티브 (하이퍼파라미터, Kubeflow, VPC)
   - Q14: 스테이크홀더 우선순위
   - Q15: PEFT (LoRA, QLoRA)
   - Q16-Q20: Googleyness & Leadership

### Jay의 강점 (답변에 활용 가능)
- **삼성물산 RAG**: Q1, Q10
- **롯데ON 추천/A/B Testing**: Q3, Q4, Q6, Q9, Q20
- **AWS SA 4년**: Q8, Q10, Q14, Q16-Q19
- **PEFT 실무**: Q15 (SageMaker JumpStart + LoRA)

### 인터뷰 전략
1. AWS 경험을 당당하게 (억지로 GCP 용어 쓰지 말 것)
2. GCP 브릿지 한 문장 (학습 의지 표현)
3. Why에 집중 (의사결정 근거)

### 추가 작업 (오후)
- **생성형 AI 디자인 패턴 섹션 추가**
  - 출처: "Generative AI Design Patterns" (O'Reilly, Lakshmanan & Hapke)
  - Jay가 읽은 한글 번역본에서 로짓 마스킹 등 발견
  - Pattern 1-6 정리: Logits Masking, Grammar, Prompt Caching, SLM, Long-Term Memory, Degradation Testing
  - 인터뷰 활용법 포함 (Q2, Q7, Q11에 연결)

- **최신 AI 엔지니어링 트렌드 섹션 추가 (중요!)**
  - ⚠️ 최근 CE 합격자 피드백: **RRK에서 이력서 기반 질문 없고, 최신 AI 기술 중심**
  - 개념/원리 이해가 핵심 (AWS 경험보다 중요)
  - 8개 트렌드 정리: Agentic RAG, Compound AI Systems, MoE, Multimodal AI, Function Calling, Prompt Engineering 고급 기법, LLM Observability, Long Context Window

- **보충 설명 추가**
  - Q6 (Feature Store): 추론 시 사용 예시, Training-Serving Skew 상세
  - Q11: 베이시안 최적화 작동 방식
  - Q15 (PEFT): LoRA 어댑터 서빙 아키텍처 (S3 저장 + 동적 로딩, vLLM/TGI)
  - Pattern 3: Prompt Caching (Semantic vs Prefix) 예시 중심 재작성
  - Quantization: FP16 vs INT8 타입 변경 이유
  - Flash Attention vs Paged Attention: 비유로 쉽게 설명
  - TTFT 최적화 방법 7가지

- **용어 명확화**
  - "데이터 그라운딩" → "그라운딩(RAG/검색 기반 근거)"
  - SLM (Small LM) vs vLLM (serving framework) vs VLM (Vision-Language Model) 구분
  - Claude MoE 여부: 공개 안 됨, 추측만 가능

- **추가 질문 답변**
  - Tree of Thoughts (ToT) 예시: 24 만들기 게임, 악수 문제, 코딩 문제 등
  - CoT (일직선) vs ToT (트리 탐색) 비교
  - 복잡한 추론/수학 문제에 ToT 유용

### 기타
- 토큰 사용량 150K 경고 규칙 AGENTS.md에 저장
- 새 세션에서도 이 규칙 기억함
- 현재 토큰: ~40K / 200K (세션 컴팩션 진행 중)

---

## Timeline

**Referral:** 2026-01-28 (Day 12 since referral)  
**First Contact:** 2026-02-09 (16:48 KST)  
**Jay 일정 회신:** 2026-02-09 (이번 주 수/목/금 가능, 2/19-20 휴가)  
**Notion 준비 완료:** 2026-02-09 (오후)  
**Next Step:** 인터뷰 일정 확정 대기 → 일정 잡히면 본격 준비 시작

<svg viewBox="0 0 800 400" xmlns="http://www.w3.org/2000/svg">
  <defs>
    <linearGradient id="bg" x1="0%" y1="0%" x2="100%" y2="100%">
      <stop offset="0%" stop-color="#0a0a2e"/>
      <stop offset="100%" stop-color="#1a1a3e"/>
    </linearGradient>
    <marker id="arrow" markerWidth="8" markerHeight="6" refX="8" refY="3" orient="auto">
      <path d="M0,0 L8,3 L0,6 Z" fill="#555"/>
    </marker>
    <marker id="arrowGreen" markerWidth="8" markerHeight="6" refX="8" refY="3" orient="auto">
      <path d="M0,0 L8,3 L0,6 Z" fill="#00d4aa"/>
    </marker>
  </defs>
  <rect width="800" height="400" rx="12" fill="url(#bg)"/>

  <!-- Title -->
  <text x="400" y="35" text-anchor="middle" font-family="NanumBarunGothic, NanumSquare, sans-serif" font-size="18" fill="#00d4aa" font-weight="bold">Iterative Magnitude Pruning (IMP) 과정</text>

  <!-- Step 1: Init -->
  <rect x="30" y="70" width="140" height="70" rx="8" fill="#00d4aa" fill-opacity="0.15" stroke="#00d4aa" stroke-width="1"/>
  <text x="100" y="96" text-anchor="middle" font-family="NanumBarunGothic, NanumSquare, sans-serif" font-size="11" fill="#00d4aa" font-weight="bold">① 랜덤 초기화</text>
  <text x="100" y="116" text-anchor="middle" font-family="NanumBarunGothic, NanumSquare, sans-serif" font-size="10" fill="#fff">θ₀ 저장</text>
  <text x="100" y="130" text-anchor="middle" font-family="NanumBarunGothic, NanumSquare, sans-serif" font-size="9" fill="#aaa">(Dense Network)</text>

  <!-- Arrow 1→2 -->
  <line x1="170" y1="105" x2="210" y2="105" stroke="#555" stroke-width="1.5" marker-end="url(#arrow)"/>

  <!-- Step 2: Train -->
  <rect x="215" y="70" width="130" height="70" rx="8" fill="#00a8ff" fill-opacity="0.15" stroke="#00a8ff" stroke-width="1"/>
  <text x="280" y="96" text-anchor="middle" font-family="NanumBarunGothic, NanumSquare, sans-serif" font-size="11" fill="#00a8ff" font-weight="bold">② 학습</text>
  <text x="280" y="116" text-anchor="middle" font-family="NanumBarunGothic, NanumSquare, sans-serif" font-size="10" fill="#fff">θ₀ → θ_T</text>
  <text x="280" y="130" text-anchor="middle" font-family="NanumBarunGothic, NanumSquare, sans-serif" font-size="9" fill="#aaa">(T epochs)</text>

  <!-- Arrow 2→3 -->
  <line x1

## LLM 시대의 프루닝: SparseGPT와 Wanda (2023)

Lottery Ticket Hypothesis가 "좋은 서브네트워크는 존재한다"는 철학적 통찰을 줬다면, 2023년에는 훨씬 더 현실적인 질문이 등장합니다. 수십억 파라미터짜리 LLM을 재학습 없이 한 번에 프루닝할 수 있을까요?

### SparseGPT (Frantar &amp; Alistarh, 2023)

SparseGPT의 핵심 아이디어는 명쾌합니다. OBS(Optimal Brain Surgeon)에서 사용한 Hessian 역행렬 기반 가중치 보정을, 행(row) 단위로 순차 처리하는 방식으로 대규모 모델에 확장한 것입니다. 기존 OBS는 전체 Hessian 역행렬 계산에 O(d³) 비용이 들어 수백 파라미터 수준에서만 실용적이었습니다. SparseGPT는 여기서 발상을 전환해, 각 열(column)을 순서대로 처리하면서 남은 가중치를 즉시 보정(update)하는 전략을 취합니다. 덕분에 GPT-175B 규모 모델도 단일 GPU에서 수 시간 내에 50% 비구조적 희소성(unstructured sparsity)을 달성할 수 있습니다.

개인적으로 가장 인상적인 부분은, **재학습(retraining) 단계가 전혀 없다**는 점입니다. 캘리브레이션 데이터 128개 샘플만으로 Hessian 근사를 수행하고, 한 번의 포워드 패스 수준 비용으로 프루닝이 완료됩니다.

### Wanda (Sun et al., 2023)

SparseGPT가 Hessian 기반 보정이라는 정교한 장치를 동원했다면, Wanda(Pruning by **W**eights **and** **A**ctivations)는 한 단계 더 단순한 길을 택합니다. 가중치 크기 |W|에 입력 활성화 노름(activation norm) ‖X‖를 곱한 중요도 점수 |W| · ‖X‖만으로 프루닝 마스크를 결정합니다. Hessian 계산도, 가중치 보정도 없습니다. 실제로 써보면 구현이 놀랍도록 간단한데, 아래 의사 코드가 거의 전부입니다:


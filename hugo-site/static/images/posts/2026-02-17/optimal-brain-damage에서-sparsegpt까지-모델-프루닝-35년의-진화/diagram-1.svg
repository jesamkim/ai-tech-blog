<svg viewBox="0 0 800 470" xmlns="http://www.w3.org/2000/svg">
  <defs>
    <linearGradient id="bg" x1="0" y1="0" x2="0" y2="1">
      <stop offset="0%" stop-color="#0a0a2e"/>
      <stop offset="100%" stop-color="#1a1a3e"/>
    </linearGradient>
    <marker id="arrow" markerWidth="8" markerHeight="6" refX="8" refY="3" orient="auto">
      <path d="#555" fill="#555" d="M0,0 L8,3 L0,6 Z"/>
    </marker>
  </defs>

  <rect width="800" height="470" rx="12" fill="url(#bg)"/>

  <!-- Title -->
  <text x="400" y="38" text-anchor="middle" font-family="NanumBarunGothic, NanumSquare, sans-serif" font-size="19" fill="#fff" font-weight="bold">모델 압축 기법 지형도</text>

  <!-- Central node -->
  <rect x="310" y="60" width="180" height="44" rx="8" fill="#00a8ff" fill-opacity="0.15" stroke="#00a8ff" stroke-width="1"/>
  <text x="400" y="78" text-anchor="middle" font-family="NanumBarunGothic, NanumSquare, sans-serif" font-size="11" fill="#00a8ff">과대 파라미터 모델</text>
  <text x="400" y="94" text-anchor="middle" font-family="NanumBarunGothic, NanumSquare, sans-serif" font-size="9" fill="#aaa">(Over-parameterized Model)</text>

  <!-- Arrows from center -->
  <line x1="280" y1="82" x2="190" y2="160" stroke="#555" stroke-width="1" marker-end="url(#arrow)"/>
  <line x1="360" y1="104" x2="310" y2="160" stroke="#555" stroke-width="1" marker-end="url(#arrow)"/>
  <line x1="440" y1="104" x2="490" y2="160" stroke="#555" stroke-width="1" marker-end="url(#arrow)"/>
  <line x1="520" y1="82" x2="610" y2="160" stroke="#555" stroke-width="1" marker-end="url(#arrow)"/>

  <!-- Pruning Box -->
  <rect x="70" y="160" width="200" height="100" rx="8" fill="#00d4aa" fill-opacity="0.15" stroke="#00d4aa" stroke-width="1.5"/>
  <text x="170" y="183" text-anchor="middle" font-family="NanumBarunGothic, NanumSquare, sans-serif" font-size="13" fill="#00d4aa" font-weight="bold">프루닝 (Pruning)</text>
  <text x="170" y="202" text-anchor="middle" font-family="NanumBarunGothic, NanumSquare, sans-serif" font-size="9" fill="#ccc">불필요한 가중치/뉴런 제거</text>
  <text x="170" y="218" text-anchor="middle" font-family="NanumBarunGothic, NanumSquare, sans-serif" font-size="9" fill="#ccc">구조적 / 비구조적</text>
  <text x="170" y="246" text-anchor="middle" font-family="NanumBarunGothic, NanumSquare, sans-serif" font-size="9" fill="#00d4aa">★ 이 글의 주인공</text>

  <!-- Quantization Box -->
  <rect x="220" y="160" width="180" height="100" rx="8" fill="#ff9900" fill-opacity="0.15" stroke="#ff9900" stroke-width="1"/>
  <text x="310" y="183" text-anchor="middle" font-family="NanumBarunGothic, NanumSquare, sans-serif" font-size="13" fill="#ff9900" font-weight="bold">양자화 (Quantization)</text>
  <text x="310" y="202" text-anchor="middle" font-family="NanumBarunGothic, NanumSquare, sans-serif" font-size="9" fill="#ccc">FP16 → INT8/INT4</text>
  <text x="310" y="218" text-anchor="middle" font-family="NanumBarunGothic, NanumSquare, sans-serif" font-size="9" fill="#ccc">비트 폭 축소</text>

  <!-- Knowledge Distillation Box -->
  <rect x="410" y="160" width="180" height="100" rx="8" fill="#aa88ff" fill-opacity="0.15" stroke="#aa88ff" stroke-width="1"/>
  <text x="500" y="183" text-anchor="middle" font-family="NanumBarunGothic, NanumSquare, sans-serif" font-size="12" fill="#aa88ff" font-weight="bold">지식 증류 (Distillation)</text>
  <text x="500" y="202" text-anchor="middle" font-family="NanumBarunGothic, NanumSquare, sans-serif" font-size="9" fill="#ccc">Teacher → Student</text>
  <text x="500" y="218" text-anchor="middle" font-family="NanumBarunGothic, NanumSquare, sans-serif" font-size="9" fill="#ccc">작은 모델로 지식 전이</text>

  <!-- NAS Box -->
  <rect x="600" y="160" width="140" height="100" rx="8" fill="#ff4444" fill-opacity="0.15" stroke="#ff4444" stroke-width="1"/>
  <text x="670" y="183" text-anchor="middle" font-family="NanumBarunGothic, NanumSquare, sans-serif" font-size="13" fill="#ff4444" font-weight="bold">NAS</text>
  <text x="670" y="202" text-anchor="middle" font-family="NanumBarunGothic, NanumSquare, sans-serif" font-

## 원조 논문들: Optimal Brain Damage &amp; Optimal Brain Surgeon (1989–1993)

모델 압축의 여러 기법 중에서도 프루닝이 가장 오래된 뿌리를 가진다고 말씀드렸는데, 그 시작점이 바로 1989년 LeCun 등이 발표한 Optimal Brain Damage(OBD)입니다.

### OBD: Hessian의 대각 근사로 "중요도"를 매기다

OBD의 핵심 아이디어는 생각보다 직관적입니다. 손실 함수(Loss function) $L$을 각 가중치 $w_i$에 대해 테일러 2차 전개(second-order Taylor expansion)하면, 특정 가중치를 0으로 만들었을 때 손실이 얼마나 증가하는지 추정할 수 있습니다. 이 추정값을 saliency라 부르며, OBD는 Hessian 행렬의 대각 성분만(diagonal approximation) 사용해 이를 근사합니다.

$$\delta L_i \approx \frac{1}{2} h_{ii} \, w_i^2 \quad \text{where } h_{ii} = \frac{\partial^2 L}{\partial w_i^2}$$

saliency가 작은 가중치부터 제거하고, 남은 네트워크를 재학습(retrain)하는 **prune → retrain 사이클**이 여기서 탄생했습니다.

### OBS: "제거 후 보정"이라는 통찰

4년 뒤 Hasselmo &amp; Stork(1993)의 Optimal Brain Surgeon(OBS)는 OBD의 대각 근사가 가중치 간 상관관계(off-diagonal terms)를 무시한다는 점을 지적했습니다. OBS는 전체 Hessian의 역행렬(full Hessian inverse) $\mathbf{H}^{-1}$을 활용해, 가중치 하나를 제거한 뒤 나머지 가중치를 최적으로 보정(weight update)합니다.

$$\delta \mathbf{w} = -\frac{w_q}{[\mathbf{H}^{-1}]_{qq}} \, \mathbf{H}^{-1} \mathbf{e}_q$$

이 보정 덕분에 OBS는 OBD보다 더 적은 가중치를 제거하고도 동등하거나 더 나은 성능을 달성했습니다. 개인적으로, 이 "제거 후 보정"이라는 아이디어가 30년 뒤 SparseGPT까지 이어진다는 점에서 OBS의 통찰은 정말 선구적이었다고 생각합니다.


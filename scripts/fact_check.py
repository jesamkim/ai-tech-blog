#!/usr/bin/env python3
"""ë¸”ë¡œê·¸ ë³¸ë¬¸ íŒ©íŠ¸ì²´í¬ (ê°•í™” ë²„ì „: ì›¹ ê²€ìƒ‰ + AI íŒë‹¨ + Dead Link ìë™ ìˆ˜ì •)"""

import json
import logging
import os
import re
import sys
from datetime import datetime
from pathlib import Path

import boto3
import requests
import yaml
from botocore.config import Config as BotoConfig

BASE_DIR = Path(__file__).resolve().parent.parent
SCRIPTS_DIR = BASE_DIR / "scripts"
LOGS_DIR = BASE_DIR / "logs"

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
)
logger = logging.getLogger("fact_check")


def load_config() -> dict:
    with open(SCRIPTS_DIR / "config.yaml") as f:
        return yaml.safe_load(f)


# â”€â”€ ì£¼ì¥ ì¶”ì¶œ íŒ¨í„´ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

CLAIM_PATTERNS = [
    (r"[^.]*\d+[\d,.]*\s*(%|í¼ì„¼íŠ¸|ë°°|ì–µ|ë§Œ|ì²œ|B|M|K|GB|TB|MB)[^.]*\.", "numeric"),
    (r"[^.]*(?:20\d{2}[-ë…„/]\s*\d{0,2})[^.]*\.", "date"),
    (r"[^.]*(?:Google|OpenAI|Meta|Microsoft|NVIDIA|Amazon|AWS|Anthropic|DeepMind|LeCun|Fei-Fei Li)\S*\s+(?:ë°œí‘œ|ì¶œì‹œ|ê³µê°œ|ë„ì…|ê°œë°œ|ì œì•ˆ|ì„¤ë¦½|ì£¼ì¥|ë…¼ë¬¸|ë°œí‘œí–ˆ|ì£¼ë„)[^.]*\.", "entity_claim"),
    (r"[^.]*(?:ë…¼ë¬¸|ì—°êµ¬|ì‹¤í—˜|ë²¤ì¹˜ë§ˆí¬|ê²°ê³¼)ì—ì„œ[^.]*\.", "research_claim"),
]


def extract_claims(text: str) -> list:
    claims = []
    seen = set()
    for pattern, claim_type in CLAIM_PATTERNS:
        for match in re.finditer(pattern, text):
            claim = match.group(0).strip()
            if claim not in seen and len(claim) > 20 and "![" not in claim and "```" not in claim:
                seen.add(claim)
                claims.append({"text": claim, "type": claim_type})
    return claims


def extract_references(text: str) -> list:
    urls = re.findall(r'https?://[^\s)\]"\']+', text)
    return list(set(urls))


# â”€â”€ URL ì ‘ê·¼ì„± í™•ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def check_url_accessible(url: str) -> dict:
    try:
        resp = requests.head(url, timeout=10, allow_redirects=True,
                             headers={"User-Agent": "Mozilla/5.0"})
        # Some sites block HEAD, try GET for 4xx/5xx
        if resp.status_code >= 400:
            resp = requests.get(url, timeout=10, allow_redirects=True,
                                headers={"User-Agent": "Mozilla/5.0"}, stream=True)
        final_url = str(resp.url)
        # Detect suspicious redirects (service page â†’ generic product/home page)
        redirected_to_generic = False
        if resp.status_code < 400 and final_url != url:
            generic_pages = ["/products/", "/products", "/index.html", "aws.amazon.com/?nc"]
            redirected_to_generic = any(g in final_url for g in generic_pages)
            if redirected_to_generic:
                logger.warning("ğŸ”€ Suspicious redirect: %s â†’ %s", url, final_url)
        accessible = resp.status_code < 400 and not redirected_to_generic
        return {"url": url, "accessible": accessible, "status": resp.status_code,
                "final_url": final_url, "redirected_to_generic": redirected_to_generic}
    except Exception as e:
        return {"url": url, "accessible": False, "error": str(e)}


# â”€â”€ Dead Link ìë™ ëŒ€ì²´ (Perplexity ê²€ìƒ‰) â”€â”€â”€â”€â”€â”€â”€â”€

def find_replacement_url(dead_url: str, context: str) -> str | None:
    """Dead linkì— ëŒ€í•´ Perplexityë¡œ ëŒ€ì²´ URL ê²€ìƒ‰"""
    api_key = os.environ.get("PERPLEXITY_API_KEY", "")
    if not api_key:
        key_path = Path.home() / ".config" / "perplexity" / "api_key"
        if key_path.exists():
            api_key = key_path.read_text().strip()
    if not api_key:
        return None

    try:
        resp = requests.post(
            "https://api.perplexity.ai/chat/completions",
            headers={"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"},
            json={
                "model": "sonar",
                "messages": [
                    {"role": "system", "content": "You find working replacement URLs for dead links. Reply with ONLY a single valid HTTPS URL on one line, nothing else. No markdown, no brackets, no citations. If you cannot find one, reply NONE."},
                    {"role": "user", "content": f"This URL returns 404: {dead_url}\nIt was referenced in this context: {context[:200]}\nFind a working official page, blog post, or documentation URL for the same topic. Reply with ONLY the URL."}
                ],
                "max_tokens": 100,
            },
            timeout=30,
        )
        if resp.status_code == 200:
            content = resp.json()["choices"][0]["message"]["content"].strip()
            # Extract URL from response
            url_match = re.search(r'https?://[^\s\)]+', content)
            if url_match and "NONE" not in content.upper():
                candidate = re.sub(r'\[\d+\]$', '', url_match.group(0).rstrip('.'))
                # Verify the replacement URL actually works
                check = check_url_accessible(candidate)
                if check.get("accessible"):
                    return candidate
                else:
                    logger.warning("  ëŒ€ì²´ URLë„ ì ‘ê·¼ ë¶ˆê°€: %s (status: %s)", candidate, check.get("status", "?"))
    except Exception as e:
        logger.warning("  ëŒ€ì²´ URL ê²€ìƒ‰ ì‹¤íŒ¨: %s", e)
    return None


def find_replacement_url_bedrock(dead_url: str, context: str, config: dict) -> str | None:
    """Bedrock Claudeë¡œ ëŒ€ì²´ URL ì¶”ì²œ (Perplexity ì‹¤íŒ¨ ì‹œ fallback)"""
    bedrock_cfg = config.get("bedrock", {})
    boto_config = BotoConfig(read_timeout=60, connect_timeout=10)
    client = boto3.client("bedrock-runtime", region_name=bedrock_cfg.get("region", "us-west-2"), config=boto_config)

    prompt = f"""ë‹¤ìŒ URLì´ 404 ì—ëŸ¬ì…ë‹ˆë‹¤: {dead_url}
ë¸”ë¡œê·¸ ë¬¸ë§¥: {context[:300]}

ê°™ì€ ì£¼ì œì˜ ê³µì‹ í˜ì´ì§€/ë¸”ë¡œê·¸/ë¬¸ì„œ ì¤‘ í™•ì‹¤íˆ ì¡´ì¬í•˜ëŠ” ëŒ€ì²´ URLì„ 1ê°œë§Œ ì¶”ì²œí•˜ì„¸ìš”.
ì¶”ì¸¡í•˜ì§€ ë§ê³ , ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” URLë§Œ ì œì‹œí•˜ì„¸ìš”.
URLë§Œ í•œ ì¤„ë¡œ ì¶œë ¥í•˜ì„¸ìš”. ëª¨ë¥´ë©´ NONEì´ë¼ê³ ë§Œ ë‹µí•˜ì„¸ìš”."""

    body = {
        "anthropic_version": "bedrock-2023-05-31",
        "max_tokens": 100,
        "temperature": 0.0,
        "messages": [{"role": "user", "content": prompt}],
    }

    try:
        response = client.invoke_model_with_response_stream(
            modelId=bedrock_cfg.get("model_id", "global.anthropic.claude-opus-4-6-v1"),
            contentType="application/json",
            accept="application/json",
            body=json.dumps(body),
        )
        chunks = []
        for event in response["body"]:
            chunk = json.loads(event["chunk"]["bytes"])
            if chunk["type"] == "content_block_delta":
                chunks.append(chunk["delta"]["text"])
        result_text = "".join(chunks).strip()

        url_match = re.search(r'https?://[^\s\)]+', result_text)
        if url_match and "NONE" not in result_text.upper():
            candidate = url_match.group(0).rstrip('.')
            check = check_url_accessible(candidate)
            if check.get("accessible"):
                return candidate
            else:
                logger.warning("  Bedrock ì¶”ì²œ URLë„ ì ‘ê·¼ ë¶ˆê°€: %s", candidate)
    except Exception as e:
        logger.warning("  Bedrock ëŒ€ì²´ URL ê²€ìƒ‰ ì‹¤íŒ¨: %s", e)
    return None


def fix_dead_links(post_path: str, url_results: list, config: dict) -> list:
    """Dead linkë¥¼ ìë™ìœ¼ë¡œ ëŒ€ì²´í•˜ê³  íŒŒì¼ ìˆ˜ì •"""
    dead_links = [u for u in url_results if not u.get("accessible")]
    if not dead_links:
        return []

    with open(post_path, encoding="utf-8") as f:
        content = f.read()

    fixes = []
    for dl in dead_links:
        dead_url = dl["url"]
        # URL ì£¼ë³€ ë¬¸ë§¥ ì¶”ì¶œ
        idx = content.find(dead_url)
        if idx < 0:
            continue
        context = content[max(0, idx - 200):idx + len(dead_url) + 200]

        logger.info("ğŸ”— Dead link ëŒ€ì²´ ì‹œë„: %s", dead_url)

        # 1ì°¨: Perplexity ì›¹ ê²€ìƒ‰
        replacement = find_replacement_url(dead_url, context)

        # 2ì°¨: Bedrock AI (Perplexity ì‹¤íŒ¨ ì‹œ)
        if not replacement:
            replacement = find_replacement_url_bedrock(dead_url, context, config)

        if replacement:
            content = content.replace(dead_url, replacement)
            fixes.append({"dead": dead_url, "replacement": replacement, "status": "fixed"})
            logger.info("  âœ… ëŒ€ì²´ ì™„ë£Œ: %s â†’ %s", dead_url, replacement)
        else:
            # ëŒ€ì²´ ì‹¤íŒ¨ â†’ í•´ë‹¹ URLì„ ì•„ì˜ˆ ì œê±°í•˜ì§€ëŠ” ì•ŠìŒ, ëŒ€ì‹  ë¡œê·¸ì— ê²½ê³ 
            fixes.append({"dead": dead_url, "replacement": None, "status": "unfixed"})
            logger.error("  âŒ ëŒ€ì²´ URLì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: %s â€” ìˆ˜ë™ í™•ì¸ í•„ìš”!", dead_url)

    if any(f["status"] == "fixed" for f in fixes):
        with open(post_path, "w", encoding="utf-8") as f:
            f.write(content)
        logger.info("ğŸ“ í¬ìŠ¤íŠ¸ íŒŒì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ (dead link %dê±´ ìˆ˜ì •)", sum(1 for f in fixes if f["status"] == "fixed"))

    unfixed = [f for f in fixes if f["status"] == "unfixed"]
    if unfixed:
        logger.error("ğŸš¨ ìˆ˜ì • ëª» í•œ dead link %dê±´ â€” ë°œí–‰ ì „ ìˆ˜ë™ í™•ì¸ í•„ìˆ˜!", len(unfixed))

    return fixes


# â”€â”€ ì†ŒìŠ¤ ê¸°ë°˜ í‚¤ì›Œë“œ ë§¤ì¹­ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def check_claim_with_source(claim: str, urls: list) -> dict:
    result = {"claim": claim, "verified": False, "source": None, "confidence": 0.0}
    keywords = re.findall(r'[A-Z][a-z]+(?:\s[A-Z][a-z]+)*|\d+[\d,.]*\s*(?:%|B|M|K|ì–µ|ë§Œ)', claim)
    if not keywords:
        return result

    for url in urls[:3]:
        try:
            resp = requests.get(url, timeout=15, headers={"User-Agent": "Mozilla/5.0"})
            if resp.status_code >= 400:
                continue
            text = resp.text[:10000].lower()
            matched = sum(1 for kw in keywords if kw.lower() in text)
            confidence = matched / len(keywords) if keywords else 0
            if confidence > result["confidence"]:
                result["confidence"] = confidence
                result["source"] = url
                if confidence >= 0.5:
                    result["verified"] = True
        except Exception:
            continue
    return result


# â”€â”€ Perplexity ì›¹ ê²€ìƒ‰ ê²€ì¦ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def search_verify_claim(claim: str) -> dict:
    api_key = os.environ.get("PERPLEXITY_API_KEY", "")
    if not api_key:
        key_path = Path.home() / ".config" / "perplexity" / "api_key"
        if key_path.exists():
            api_key = key_path.read_text().strip()
    if not api_key:
        return {"method": "search", "verified": None, "reason": "API key not found"}

    try:
        resp = requests.post(
            "https://api.perplexity.ai/chat/completions",
            headers={"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"},
            json={
                "model": "sonar",
                "messages": [
                    {"role": "system", "content": "You are a fact-checker. Verify the following claim. Reply in JSON: {\"verified\": true/false/null, \"reason\": \"brief explanation\", \"confidence\": 0.0-1.0}"},
                    {"role": "user", "content": f"Verify this claim: {claim}"}
                ],
                "max_tokens": 300,
            },
            timeout=30,
        )
        if resp.status_code == 200:
            content = resp.json()["choices"][0]["message"]["content"]
            json_match = re.search(r'\{.*?\}', content, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                result["method"] = "search"
                return result
        return {"method": "search", "verified": None, "reason": f"API error {resp.status_code}"}
    except Exception as e:
        return {"method": "search", "verified": None, "reason": str(e)}


# â”€â”€ Bedrock AI íŒë‹¨ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def ai_judge_claims(claims: list, config: dict) -> list:
    if not claims:
        return []

    bedrock_cfg = config.get("bedrock", {})
    boto_config = BotoConfig(read_timeout=120, connect_timeout=10)
    client = boto3.client("bedrock-runtime", region_name=bedrock_cfg.get("region", "us-west-2"), config=boto_config)

    claims_text = "\n".join([f"{i+1}. [{c['type']}] {c['text']}" for i, c in enumerate(claims)])

    prompt = f"""ë‹¹ì‹ ì€ AI/ML ê¸°ìˆ  ë¸”ë¡œê·¸ì˜ íŒ©íŠ¸ì²´ì»¤ì…ë‹ˆë‹¤.
ì•„ë˜ ì£¼ì¥ë“¤ì˜ ì‚¬ì‹¤ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ì„¸ìš”.

## ì£¼ì¥ ëª©ë¡
{claims_text}

## íŒë‹¨ ê¸°ì¤€
- ë‚ ì§œ, ìˆ˜ì¹˜, ì¸ë¬¼, ê¸°ê´€ëª…ì´ ì •í™•í•œì§€
- ê¸°ìˆ ì  ì„¤ëª…ì´ ì˜¬ë°”ë¥¸ì§€
- ì¸ê³¼ê´€ê³„ê°€ ë…¼ë¦¬ì ì¸ì§€

## ì¶œë ¥ í˜•ì‹ (JSON ë°°ì—´)
[
  {{"id": 1, "verdict": "correct|incorrect|uncertain", "issue": "ë¬¸ì œê°€ ìˆìœ¼ë©´ ê°„ëµíˆ ì„¤ëª…, ì—†ìœ¼ë©´ null"}}
]

JSON ë°°ì—´ë§Œ ì¶œë ¥í•˜ì„¸ìš”."""

    body = {
        "anthropic_version": "bedrock-2023-05-31",
        "max_tokens": 1024,
        "temperature": 0.0,
        "messages": [{"role": "user", "content": prompt}],
    }

    try:
        response = client.invoke_model_with_response_stream(
            modelId=bedrock_cfg.get("model_id", "global.anthropic.claude-opus-4-6-v1"),
            contentType="application/json",
            accept="application/json",
            body=json.dumps(body),
        )
        chunks = []
        for event in response["body"]:
            chunk = json.loads(event["chunk"]["bytes"])
            if chunk["type"] == "content_block_delta":
                chunks.append(chunk["delta"]["text"])
        result_text = "".join(chunks)

        json_match = re.search(r'\[.*\]', result_text, re.DOTALL)
        if json_match:
            return json.loads(json_match.group())
    except Exception as e:
        logger.warning("AI íŒ©íŠ¸ì²´í¬ ì‹¤íŒ¨: %s", e)

    return []


# â”€â”€ ë©”ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def fact_check_post(post_path: str, config: dict = None, auto_fix: bool = True) -> dict:
    if config is None:
        config = load_config()
    fc_cfg = config.get("fact_check", {})
    if not fc_cfg.get("enabled", True):
        logger.info("íŒ©íŠ¸ì²´í¬ ë¹„í™œì„±í™”")
        return {"enabled": False}

    with open(post_path, encoding="utf-8") as f:
        content = f.read()

    body = re.sub(r"^---.*?---", "", content, count=1, flags=re.DOTALL).strip()

    # 1. ì£¼ì¥ ì¶”ì¶œ
    claims = extract_claims(body)
    max_claims = fc_cfg.get("max_claims_to_check", 10)
    claims = claims[:max_claims]
    logger.info("íŒ©íŠ¸ì²´í¬ ëŒ€ìƒ: %dê±´", len(claims))

    # 2. URL ì ‘ê·¼ì„±
    urls = extract_references(content)
    logger.info("ì°¸ì¡° URL: %dê±´", len(urls))
    url_results = [check_url_accessible(u) for u in urls]
    dead_links = [u for u in url_results if not u.get("accessible")]
    if dead_links:
        logger.warning("âš ï¸ Dead links: %dê±´", len(dead_links))

    # 2.3 Reference ë¼ë²¨â†”URL ì¢…ë¥˜ ë¶ˆì¼ì¹˜ ê²€ì‚¬
    ref_mismatches = []
    ref_pattern = re.findall(r'(\S+(?:ê³µì‹ ë¬¸ì„œ|Blog|ë…¼ë¬¸|GitHub)[^
]*)(https?://[^\s\)\]]+)', body)
    for label_line, url in ref_pattern:
        is_docs = "docs.aws" in url or "documentation" in url
        is_blog = "/blogs/" in url or "/blog/" in url
        is_arxiv = "arxiv.org" in url
        is_github = "github.com" in url
        mismatch = None
        if "ê³µì‹ ë¬¸ì„œ" in label_line and not is_docs:
            mismatch = f"'ê³µì‹ ë¬¸ì„œ' label but URL is not docs: {url}"
        if "Blog" in label_line and not is_blog:
            mismatch = f"'Blog' label but URL is not a blog: {url}"
        if mismatch:
            ref_mismatches.append(mismatch)
            logger.warning("âš ï¸ ë¼ë²¨ ë¶ˆì¼ì¹˜: %s", mismatch)

    # 2.5 Dead Link ìë™ ìˆ˜ì •
    link_fixes = []
    if dead_links and auto_fix:
        link_fixes = fix_dead_links(post_path, url_results, config)
        # ìˆ˜ì • ëª» í•œ dead linkê°€ ìˆìœ¼ë©´ ë°œí–‰ ì°¨ë‹¨
        unfixed = [f for f in link_fixes if f["status"] == "unfixed"]
        if unfixed:
            logger.error("ğŸš¨ Dead link %dê±´ ìˆ˜ì • ì‹¤íŒ¨ â€” ë°œí–‰ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤!", len(unfixed))

    # 3. ì†ŒìŠ¤ ê¸°ë°˜ í‚¤ì›Œë“œ ë§¤ì¹­
    claim_results = []
    threshold = fc_cfg.get("confidence_threshold", 0.6)
    for c in claims:
        r = check_claim_with_source(c["text"], urls) if urls else {"claim": c["text"], "verified": False, "confidence": 0.0}
        r["type"] = c["type"]
        claim_results.append(r)

    # 4. Perplexity ì›¹ ê²€ìƒ‰ (ë¯¸í™•ì¸ ì£¼ì¥ë§Œ, ìµœëŒ€ 5ê±´)
    unverified = [c for c in claim_results if not c.get("verified")]
    search_results = []
    for c in unverified[:5]:
        sr = search_verify_claim(c["claim"])
        search_results.append({"claim": c["claim"][:80], **sr})
        if sr.get("verified") is True:
            c["verified"] = True
            c["confidence"] = max(c.get("confidence", 0), sr.get("confidence", 0.7))
            c["verification_method"] = "web_search"
    logger.info("ì›¹ ê²€ìƒ‰ ê²€ì¦: %dê±´ ì‹œë„", len(search_results))

    # 5. Bedrock AI íŒë‹¨ (ì—¬ì „íˆ ë¯¸í™•ì¸ì¸ ì£¼ì¥)
    still_unverified = [c for c in claim_results if not c.get("verified")]
    if still_unverified:
        logger.info("AI íŒë‹¨: %dê±´", len(still_unverified))
        ai_results = ai_judge_claims(
            [{"text": c["claim"], "type": c.get("type", "")} for c in still_unverified],
            config
        )
        for ai_r in ai_results:
            idx = ai_r.get("id", 0) - 1
            if 0 <= idx < len(still_unverified):
                c = still_unverified[idx]
                verdict = ai_r.get("verdict", "uncertain")
                c["ai_verdict"] = verdict
                c["ai_issue"] = ai_r.get("issue")
                if verdict == "correct":
                    c["verified"] = True
                    c["confidence"] = max(c.get("confidence", 0), 0.7)
                    c["verification_method"] = "ai_judge"
                elif verdict == "incorrect":
                    c["flagged"] = True
                    c["flag_reason"] = ai_r.get("issue", "AIê°€ ì˜¤ë¥˜ë¡œ íŒë‹¨")

    # ìµœì¢… í”Œë˜ê·¸
    for c in claim_results:
        if "flagged" not in c:
            c["flagged"] = not c.get("verified") and c.get("confidence", 0) < threshold

    flagged = [c for c in claim_results if c.get("flagged")]

    result = {
        "post": str(post_path),
        "timestamp": datetime.now().isoformat(),
        "total_claims": len(claims),
        "verified_claims": sum(1 for c in claim_results if c.get("verified")),
        "flagged_claims": len(flagged),
        "dead_links": len(dead_links),
        "dead_links_fixed": sum(1 for f in link_fixes if f["status"] == "fixed"),
        "dead_links_unfixed": sum(1 for f in link_fixes if f["status"] == "unfixed"),
        "link_fixes": link_fixes,
        "claims": claim_results,
        "search_results": search_results,
        "urls": url_results,
    }

    LOGS_DIR.mkdir(parents=True, exist_ok=True)
    today = datetime.now().strftime("%Y-%m-%d")
    out = LOGS_DIR / f"fact_check_{today}.json"
    with open(out, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    logger.info("íŒ©íŠ¸ì²´í¬ ê²°ê³¼: %d/%d í™•ì¸, %dê±´ í”Œë˜ê·¸, %d dead links (%d fixed, %d unfixed)",
                result["verified_claims"], result["total_claims"], result["flagged_claims"],
                result["dead_links"], result["dead_links_fixed"], result["dead_links_unfixed"])
    if flagged:
        for c in flagged:
            reason = c.get("flag_reason", c.get("ai_issue", "ë¯¸í™•ì¸"))
            logger.warning("âš ï¸ [%s] %s â€” %s", c.get("ai_verdict", "?"), c["claim"][:60], reason)
    return result


def main():
    import argparse
    parser = argparse.ArgumentParser(description="ë¸”ë¡œê·¸ íŒ©íŠ¸ì²´í¬")
    parser.add_argument("--post", required=True, help="í¬ìŠ¤íŠ¸ íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--no-fix", action="store_true", help="Dead link ìë™ ìˆ˜ì • ë¹„í™œì„±í™”")
    args = parser.parse_args()

    result = fact_check_post(args.post, auto_fix=not args.no_fix)
    print(json.dumps(result, ensure_ascii=False, indent=2))

    # Dead link ë¯¸ìˆ˜ì • ìˆìœ¼ë©´ exit code 2
    if result.get("dead_links_unfixed", 0) > 0:
        sys.exit(2)
    # í”Œë˜ê·¸ëœ ì£¼ì¥ ìˆìœ¼ë©´ exit code 1
    if result.get("flagged_claims", 0) > 0:
        sys.exit(1)
    sys.exit(0)


if __name__ == "__main__":
    main()
